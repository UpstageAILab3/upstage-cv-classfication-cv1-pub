{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# augmentation을 적용한 함수 dl모델을 제작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 증강 함수\n",
    "- A.Resize(height=256, width=256) : 사이즈 변경\n",
    "- A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) : 이미지 픽셀 정규화\n",
    "- A.CoarseDropout(max_holes=16, max_height=16, max_width=16, min_holes=1, min_height=16, min_width=16) : 픽셀 드롭아웃(구멍뚫기)\n",
    "- A.Rotate(limit=(115, 115), border_mode=cv2.BORDER_CONSTANT,value=[255, 255, 255], p=1.0) : 비율 유지하여 회전, 빈공간 흰색\n",
    "- A.VerticalFlip(always_apply=False, p=0.5) : 좌우반전\n",
    "- A.HorizontalFlip(always_apply=False, p=1.0) : 상하반전\n",
    "- A.Blur(always_apply=True, p=1.0, blur_limit=(3, 7)) : 블러\n",
    "- A.GaussNoise(always_apply=False, p=1.0, var_limit=(10.0, 50.0)) : 가우시안, 되는지 의문\n",
    "- A.Downscale(always_apply=False, p=1.0, scale_min=0.35, scale_max=0.35, interpolation=0) : 픽셀다운, 모자이크\n",
    "- 믹스 추가필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드를 고정합니다.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스를 정의합니다.\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one epoch 학습을 위한 함수입니다.\n",
    "def train_one_epoch(loader, vaild_loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    vaild_preds_list = []\n",
    "    vaild_target_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.float().to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    # vaildation set predict\n",
    "    model.eval()\n",
    "    for image, target in tqdm(vaild_loader):\n",
    "        image = image.float().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(image)\n",
    "        vaild_preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        vaild_target_list.extend(target.detach().cpu().numpy())\n",
    "    vaild_acc = accuracy_score(vaild_target_list, vaild_preds_list)\n",
    "    vaild_f1 = f1_score(vaild_target_list, vaild_preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "        \"vaild_acc\": vaild_acc,\n",
    "        \"vaild_f1\": vaild_f1,\n",
    "    }\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vaild_preds_list = []\n",
    "# vaild_target_list = []\n",
    "\n",
    "# model.eval()\n",
    "# for image, target in tqdm(vaild_loader):\n",
    "#     image = image.float().to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         preds = model(image)\n",
    "#     vaild_preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "#     vaild_target_list.extend(target.detach().cpu().numpy())\n",
    "# vaild_acc = accuracy_score(vaild_target_list, preds_list)\n",
    "# vaild_f1 = f1_score(vaild_target_list, preds_list, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# data config\n",
    "data_path = '../datasets_fin/'\n",
    "\n",
    "# model config\n",
    "model_name = 'resnet34' # 'resnet50' 'efficientnet-b0', ...\n",
    "\n",
    "# training config\n",
    "img_size = 256\n",
    "LR = 1e-3\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일단 단일 augmentation들로 확인\n",
    "- 성능향상시 다양한 데이터증강을 조합하여 추가학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation을 위한 transform 코드\n",
    "trn_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "trn_transform_coarseDropout = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    A.CoarseDropout(max_holes=30, max_height=16, max_width=16, min_holes=10, min_height=16, min_width=16, p=1),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "trn_transform_rotate45 = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    A.Rotate(limit=(45, 45), border_mode=cv2.BORDER_CONSTANT,value=[255, 255, 255], p=1),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "trn_transform_rotate90 = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), \n",
    "    A.Rotate(limit=(90, 90), border_mode=cv2.BORDER_CONSTANT,value=[255, 255, 255], p=1),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "trn_transform_rotate135 = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    A.Rotate(limit=(135, 135), border_mode=cv2.BORDER_CONSTANT,value=[255, 255, 255], p=1),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "trn_transform_rotate180 = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    A.Rotate(limit=(180, 180), border_mode=cv2.BORDER_CONSTANT,value=[255, 255, 255], p=1),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "trn_transform_rotate225 = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    A.Rotate(limit=(225, 225), border_mode=cv2.BORDER_CONSTANT,value=[255, 255, 255], p=1.0),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "trn_transform_rotate270 = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    A.Rotate(limit=(270, 270), border_mode=cv2.BORDER_CONSTANT,value=[255, 255, 255], p=1.0),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "trn_transform_rotate315 = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    A.Rotate(limit=(315, 315), border_mode=cv2.BORDER_CONSTANT,value=[255, 255, 255], p=1.0),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "trn_transform_verticalflip = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    A.VerticalFlip(always_apply=False, p=1.0),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "trn_transform_horizontalflip = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    A.HorizontalFlip(always_apply=False, p=1.0),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "trn_transform_blur = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    A.Blur(always_apply=True, p=1.0, blur_limit=(3, 3)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "trn_transform_gaussnoise = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    A.GaussNoise(always_apply=False, p=1.0, var_limit=(500.0, 500.0)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "trn_transform_downscale = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    A.Downscale(always_apply=False, p=1.0, scale_min=0.5, scale_max=0.5, interpolation=0),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "transform_list = [trn_transform, trn_transform_coarseDropout, trn_transform_rotate45, trn_transform_rotate90, trn_transform_rotate135, trn_transform_rotate180, trn_transform_rotate225,\n",
    "                  trn_transform_rotate270, trn_transform_rotate315, trn_transform_verticalflip, trn_transform_horizontalflip, trn_transform_blur, trn_transform_gaussnoise,\n",
    "                  trn_transform_downscale]\n",
    "\n",
    "# test image 변환을 위한 transform 코드\n",
    "tst_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 정의를 위한 함수\n",
    "def make_trn_dataset(transform_list, csv, path):\n",
    "    trn_dataset = ImageDataset(\n",
    "        csv,\n",
    "        path,\n",
    "        transform=transform_list[0]\n",
    "    )\n",
    "    for transform in transform_list[1:]:\n",
    "        trn_dataset2 = ImageDataset(\n",
    "            csv,\n",
    "            path,\n",
    "            transform=transform\n",
    "        )\n",
    "        trn_dataset = ConcatDataset([trn_dataset, trn_dataset2])\n",
    "\n",
    "    return trn_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vaildation set 정의\n",
    "def make_vaild_dataset(transform_list, csv, path):\n",
    "    trn_dataset = ImageDataset(\n",
    "        csv,\n",
    "        path,\n",
    "        transform=transform_list[0]\n",
    "    )\n",
    "    for transform in transform_list[1:]:\n",
    "        trn_dataset2 = ImageDataset(\n",
    "            csv,\n",
    "            path,\n",
    "            transform=transform\n",
    "        )\n",
    "        trn_dataset = ConcatDataset([trn_dataset, trn_dataset2])\n",
    "\n",
    "    return trn_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17584 2198 157\n"
     ]
    }
   ],
   "source": [
    "trn_dataset = make_trn_dataset(transform_list, \"../datasets_fin/divided_train.csv\", \"../datasets_fin/train/\")\n",
    "vaild_dataset = make_vaild_dataset(transform_list, \"../datasets_fin/vaild.csv\", \"../datasets_fin/train/\",)\n",
    "tst_dataset = ImageDataset(\n",
    "    \"../datasets_fin/test.csv\",\n",
    "    \"../datasets_fin/train/\",\n",
    "    transform=tst_transform\n",
    ")\n",
    "print(len(trn_dataset), len(vaild_dataset), len(tst_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 정의\n",
    "trn_loader = DataLoader(\n",
    "    trn_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    # shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "vaild_loader = DataLoader(\n",
    "    vaild_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    # shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "tst_loader = DataLoader(\n",
    "    tst_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = timm.create_model(\n",
    "    model_name,\n",
    "    pretrained=True,\n",
    "    num_classes=17\n",
    ").to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6891: 100%|██████████| 550/550 [01:47<00:00,  5.13it/s]\n",
      "100%|██████████| 69/69 [00:10<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train_loss: 0.8928\n",
      "train_acc: 0.7000\n",
      "train_f1: 0.6780\n",
      "vaild_acc: 0.7520\n",
      "vaild_f1: 0.7411\n",
      "epoch: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1688: 100%|██████████| 550/550 [01:47<00:00,  5.11it/s]\n",
      "100%|██████████| 69/69 [00:10<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train_loss: 0.4026\n",
      "train_acc: 0.8569\n",
      "train_f1: 0.8544\n",
      "vaild_acc: 0.7630\n",
      "vaild_f1: 0.7412\n",
      "epoch: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3147: 100%|██████████| 550/550 [01:47<00:00,  5.10it/s]\n",
      "100%|██████████| 69/69 [00:11<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train_loss: 0.3123\n",
      "train_acc: 0.8921\n",
      "train_f1: 0.8930\n",
      "vaild_acc: 0.8330\n",
      "vaild_f1: 0.8329\n",
      "epoch: 2.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3922: 100%|██████████| 550/550 [01:47<00:00,  5.12it/s]\n",
      "100%|██████████| 69/69 [00:10<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train_loss: 0.2768\n",
      "train_acc: 0.9054\n",
      "train_f1: 0.9074\n",
      "vaild_acc: 0.8335\n",
      "vaild_f1: 0.8392\n",
      "epoch: 3.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1966: 100%|██████████| 550/550 [01:47<00:00,  5.10it/s]\n",
      "100%|██████████| 69/69 [00:10<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "train_loss: 0.2525\n",
      "train_acc: 0.9134\n",
      "train_f1: 0.9155\n",
      "vaild_acc: 0.8285\n",
      "vaild_f1: 0.8367\n",
      "epoch: 4.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2137: 100%|██████████| 550/550 [01:47<00:00,  5.11it/s]\n",
      "100%|██████████| 69/69 [00:10<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "train_loss: 0.2427\n",
      "train_acc: 0.9187\n",
      "train_f1: 0.9226\n",
      "vaild_acc: 0.8044\n",
      "vaild_f1: 0.8145\n",
      "epoch: 5.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6473: 100%|██████████| 550/550 [01:47<00:00,  5.10it/s]\n",
      "100%|██████████| 69/69 [00:10<00:00,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "train_loss: 0.2409\n",
      "train_acc: 0.9194\n",
      "train_f1: 0.9220\n",
      "vaild_acc: 0.8203\n",
      "vaild_f1: 0.8257\n",
      "epoch: 6.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "early_stop = 0\n",
    "best_model = model\n",
    "best_epoch = 0\n",
    "best_vaild_f1 = 0\n",
    "for epoch in range(EPOCHS):\n",
    "   if early_stop > 2:\n",
    "      torch.save(best_model, '../model/best_augmentation_'+str(img_size)+'_'+str(best_epoch)+'.pt')\n",
    "      break\n",
    "   ret = train_one_epoch(trn_loader, vaild_loader, model, optimizer, loss_fn, device=device)\n",
    "   ret['epoch'] = epoch\n",
    "   if best_vaild_f1 < ret['vaild_f1']:\n",
    "      best_vaild_f1 = ret['vaild_f1']\n",
    "      best_model = model\n",
    "      best_epoch = epoch+1\n",
    "      early_stop = 0\n",
    "   else:\n",
    "      early_stop += 1\n",
    "\n",
    "   print(early_stop)\n",
    "\n",
    "   log = \"\"\n",
    "   for k, v in ret.items():\n",
    "      log += f\"{k}: {v:.4f}\\n\"\n",
    "   print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5597: 100%|██████████| 550/550 [01:11<00:00,  7.70it/s]\n",
      "100%|██████████| 69/69 [00:07<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0916\n",
      "train_acc: 0.9693\n",
      "train_f1: 0.9679\n",
      "vaild_acc: 0.8430\n",
      "vaild_f1: 0.8393\n",
      "epoch: 0.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0250: 100%|██████████| 550/550 [01:11<00:00,  7.67it/s]\n",
      "100%|██████████| 69/69 [00:07<00:00,  8.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0764\n",
      "train_acc: 0.9738\n",
      "train_f1: 0.9731\n",
      "vaild_acc: 0.8749\n",
      "vaild_f1: 0.8683\n",
      "epoch: 1.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0036: 100%|██████████| 550/550 [01:11<00:00,  7.68it/s]\n",
      "100%|██████████| 69/69 [00:07<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0566\n",
      "train_acc: 0.9816\n",
      "train_f1: 0.9805\n",
      "vaild_acc: 0.8726\n",
      "vaild_f1: 0.8645\n",
      "epoch: 2.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0695: 100%|██████████| 550/550 [01:11<00:00,  7.69it/s]\n",
      "100%|██████████| 69/69 [00:07<00:00,  8.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0490\n",
      "train_acc: 0.9823\n",
      "train_f1: 0.9813\n",
      "vaild_acc: 0.8763\n",
      "vaild_f1: 0.8727\n",
      "epoch: 3.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0056: 100%|██████████| 550/550 [01:11<00:00,  7.66it/s]\n",
      "100%|██████████| 69/69 [00:07<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0494\n",
      "train_acc: 0.9838\n",
      "train_f1: 0.9830\n",
      "vaild_acc: 0.8926\n",
      "vaild_f1: 0.8902\n",
      "epoch: 4.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0470: 100%|██████████| 550/550 [01:11<00:00,  7.68it/s]\n",
      "100%|██████████| 69/69 [00:07<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0381\n",
      "train_acc: 0.9870\n",
      "train_f1: 0.9864\n",
      "vaild_acc: 0.8699\n",
      "vaild_f1: 0.8626\n",
      "epoch: 5.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1431: 100%|██████████| 550/550 [01:11<00:00,  7.68it/s]\n",
      "100%|██████████| 69/69 [00:07<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0364\n",
      "train_acc: 0.9883\n",
      "train_f1: 0.9878\n",
      "vaild_acc: 0.8813\n",
      "vaild_f1: 0.8796\n",
      "epoch: 6.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1292: 100%|██████████| 550/550 [01:11<00:00,  7.67it/s]\n",
      "100%|██████████| 69/69 [00:07<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0369\n",
      "train_acc: 0.9882\n",
      "train_f1: 0.9876\n",
      "vaild_acc: 0.8976\n",
      "vaild_f1: 0.8944\n",
      "epoch: 7.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0112: 100%|██████████| 550/550 [01:11<00:00,  7.68it/s]\n",
      "100%|██████████| 69/69 [00:07<00:00,  8.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0243\n",
      "train_acc: 0.9931\n",
      "train_f1: 0.9927\n",
      "vaild_acc: 0.8854\n",
      "vaild_f1: 0.8815\n",
      "epoch: 8.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0854: 100%|██████████| 550/550 [01:11<00:00,  7.65it/s]\n",
      "100%|██████████| 69/69 [00:07<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0336\n",
      "train_acc: 0.9888\n",
      "train_f1: 0.9879\n",
      "vaild_acc: 0.8913\n",
      "vaild_f1: 0.8860\n",
      "epoch: 9.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0053: 100%|██████████| 550/550 [01:11<00:00,  7.68it/s]\n",
      "100%|██████████| 69/69 [00:07<00:00,  8.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0204\n",
      "train_acc: 0.9928\n",
      "train_f1: 0.9924\n",
      "vaild_acc: 0.9054\n",
      "vaild_f1: 0.9049\n",
      "epoch: 10.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0046: 100%|██████████| 550/550 [01:11<00:00,  7.68it/s]\n",
      "100%|██████████| 69/69 [00:07<00:00,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0261\n",
      "train_acc: 0.9917\n",
      "train_f1: 0.9913\n",
      "vaild_acc: 0.8653\n",
      "vaild_f1: 0.8483\n",
      "epoch: 11.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1145: 100%|██████████| 550/550 [01:11<00:00,  7.72it/s]\n",
      "100%|██████████| 69/69 [00:07<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0270\n",
      "train_acc: 0.9917\n",
      "train_f1: 0.9914\n",
      "vaild_acc: 0.8908\n",
      "vaild_f1: 0.8860\n",
      "epoch: 12.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2038: 100%|██████████| 550/550 [01:11<00:00,  7.67it/s]\n",
      "100%|██████████| 69/69 [00:07<00:00,  8.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0184\n",
      "train_acc: 0.9951\n",
      "train_f1: 0.9948\n",
      "vaild_acc: 0.8949\n",
      "vaild_f1: 0.8883\n",
      "epoch: 13.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0041: 100%|██████████| 550/550 [01:11<00:00,  7.66it/s]\n",
      "100%|██████████| 69/69 [00:07<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0329\n",
      "train_acc: 0.9894\n",
      "train_f1: 0.9890\n",
      "vaild_acc: 0.8908\n",
      "vaild_f1: 0.8887\n",
      "epoch: 14.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0074: 100%|██████████| 550/550 [01:11<00:00,  7.70it/s]\n",
      "100%|██████████| 69/69 [00:07<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0207\n",
      "train_acc: 0.9938\n",
      "train_f1: 0.9936\n",
      "vaild_acc: 0.8922\n",
      "vaild_f1: 0.8881\n",
      "epoch: 15.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0373: 100%|██████████| 550/550 [01:11<00:00,  7.64it/s]\n",
      "100%|██████████| 69/69 [00:08<00:00,  8.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0196\n",
      "train_acc: 0.9943\n",
      "train_f1: 0.9942\n",
      "vaild_acc: 0.8885\n",
      "vaild_f1: 0.8831\n",
      "epoch: 16.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0022: 100%|██████████| 550/550 [01:11<00:00,  7.67it/s]\n",
      "100%|██████████| 69/69 [00:07<00:00,  8.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0164\n",
      "train_acc: 0.9953\n",
      "train_f1: 0.9950\n",
      "vaild_acc: 0.8863\n",
      "vaild_f1: 0.8818\n",
      "epoch: 17.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0001: 100%|██████████| 550/550 [01:11<00:00,  7.65it/s]\n",
      "100%|██████████| 69/69 [00:07<00:00,  8.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0191\n",
      "train_acc: 0.9947\n",
      "train_f1: 0.9943\n",
      "vaild_acc: 0.8913\n",
      "vaild_f1: 0.8868\n",
      "epoch: 18.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0001: 100%|██████████| 550/550 [01:11<00:00,  7.70it/s]\n",
      "100%|██████████| 69/69 [00:07<00:00,  8.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.0201\n",
      "train_acc: 0.9931\n",
      "train_f1: 0.9927\n",
      "vaild_acc: 0.8922\n",
      "vaild_f1: 0.8877\n",
      "epoch: 19.0000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    ret = train_one_epoch(trn_loader, vaild_loader, model, optimizer, loss_fn, device=device)\n",
    "    ret['epoch'] = epoch\n",
    "\n",
    "    log = \"\"\n",
    "    for k, v in ret.items():\n",
    "      log += f\"{k}: {v:.4f}\\n\"\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../model/best_augmentation_256_4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  5.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9171974522292994, 0.913738222561752)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test 데이터 확인\n",
    "tst_preds_list = []\n",
    "tst_target_list = []\n",
    "\n",
    "model.eval()\n",
    "for image, target in tqdm(tst_loader):\n",
    "    image = image.float().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(image)\n",
    "    tst_preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "    tst_target_list.extend(target.detach().cpu().numpy())\n",
    "tst_acc = accuracy_score(tst_target_list, tst_preds_list)\n",
    "tst_f1 = f1_score(tst_target_list, tst_preds_list, average='macro')\n",
    "\n",
    "tst_acc, tst_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImageDataset(\n",
    "    \"../datasets_fin/sample_submission.csv\",\n",
    "    \"../datasets_fin/test/\",\n",
    "    transform=tst_transform\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:14<00:00,  7.05it/s]\n"
     ]
    }
   ],
   "source": [
    "preds_list = []\n",
    "\n",
    "model.eval()\n",
    "for image, _ in tqdm(test_loader):\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(test_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_df = pd.read_csv(\"../datasets_fin/sample_submission.csv\")\n",
    "assert (sample_submission_df['ID'] == pred_df['ID']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(\"../output/augmentation_pred_256_\"+str(best_epoch)+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0008fdb22ddce0ce.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00091bffdffd83de.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00396fbc1f6cc21d.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00471f8038d9c4b6.jpg</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00901f504008d884.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID  target\n",
       "0  0008fdb22ddce0ce.jpg       2\n",
       "1  00091bffdffd83de.jpg      12\n",
       "2  00396fbc1f6cc21d.jpg       5\n",
       "3  00471f8038d9c4b6.jpg       6\n",
       "4  00901f504008d884.jpg       2"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
